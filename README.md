# ğŸ¦ Banking Modern Data Stack Pipeline

<div align="center">

![Snowflake](https://img.shields.io/badge/Snowflake-29B5E8?logo=snowflake&logoColor=white&style=for-the-badge)
![DBT](https://img.shields.io/badge/dbt-FF694B?logo=dbt&logoColor=white&style=for-the-badge)
![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-017CEE?logo=apacheairflow&logoColor=white&style=for-the-badge)
![Apache Kafka](https://img.shields.io/badge/Apache%20Kafka-231F20?logo=apachekafka&logoColor=white&style=for-the-badge)
![Debezium](https://img.shields.io/badge/Debezium-EF3B2D?logo=apache&logoColor=white&style=for-the-badge)
![Python](https://img.shields.io/badge/Python-3776AB?logo=python&logoColor=white&style=for-the-badge)
![Docker](https://img.shields.io/badge/Docker-2496ED?logo=docker&logoColor=white&style=for-the-badge)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-316192?logo=postgresql&logoColor=white&style=for-the-badge)
![MinIO](https://img.shields.io/badge/MinIO-C72E49?logo=minio&logoColor=white&style=for-the-badge)

**An enterprise-grade, end-to-end real-time data engineering pipeline simulating a modern banking system**

[Features](#-key-features) â€¢ [Architecture](#ï¸-architecture) â€¢ [Tech Stack](#-tech-stack) â€¢ [Quick Start](#-quick-start) â€¢ [Pipeline Flow](#-pipeline-flow)

</div>

---

## ğŸ“Œ Project Overview

This project demonstrates a **production-ready modern data stack** implementation for the **banking domain**, showcasing:

- âœ… **Real-time Change Data Capture (CDC)** from transactional databases
- âœ… **Event-driven architecture** with streaming data pipelines
- âœ… **Cloud data warehousing** with dimensional modeling
- âœ… **Slowly Changing Dimensions (SCD Type-2)** for historical tracking
- âœ… **Automated orchestration** and data quality testing
- âœ… **CI/CD integration** with automated deployment workflows

> ğŸ’¡ **Use Case**: Imagine you're a data engineer at a bank that needs to track customer accounts, transactions, and balance changes in real-time while maintaining historical records for compliance and analytics.

---

## ğŸ—„ï¸ Database Schema

The OLTP system consists of three core entities representing a simplified banking system:

![Database Schema](schema.png)

### **Schema Design Highlights:**

| Table | Purpose | Key Relationships |
|-------|---------|------------------|
| **`customers`** | Stores customer demographic information | One-to-Many â†’ `accounts` |
| **`accounts`** | Banking account details (balance, type, currency) | Many-to-One â†’ `customers`<br>One-to-Many â†’ `transactions` |
| **`transactions`** | Financial transaction records | Many-to-One â†’ `accounts` |

**Key Features:**
- **Referential Integrity**: Foreign key constraints ensure data consistency
- **Timestamps**: All tables include `created_at` for temporal tracking
- **Account Types**: Supports checking, savings, credit accounts
- **Transaction Types**: Captures deposits, withdrawals, transfers, payments
- **Multi-Currency**: Accounts can hold different currencies (USD, EUR, GBP, etc.)

---

## ğŸ—ï¸ Architecture  

### **High-Level System Design**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Gen   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚PostgreSQLâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Kafka   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  MinIO    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Snowflake   â”‚
â”‚  (Faker)    â”‚         â”‚  (OLTP)  â”‚         â”‚+Debeziumâ”‚         â”‚  (S3)     â”‚         â”‚  (OLAP)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                    â”‚                    â”‚                      â”‚
                              â”‚                    â”‚                    â”‚                      â”‚
                        CDC Capture          Stream Events         Object Store           Data Warehouse
                        (WAL Logs)          (JSON/Avro)            (Parquet)              (Bronze/Silver/Gold)
                                                                         â”‚                      â”‚
                                                                         â”‚                      â”‚
                                                                         â–¼                      â–¼
                                                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                                   â”‚ Airflow  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚    dbt     â”‚
                                                                   â”‚  (DAGs)  â”‚         â”‚ (Transform)â”‚
                                                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                   Orchestration        Modeling & Tests
```

### **Pipeline Flow Explained:**

1. **ğŸ“Š Data Generation Layer**
   - Python Faker generates realistic banking data (customers, accounts, transactions)
   - Simulates real-world scenarios: account openings, deposits, withdrawals, transfers
   - Configurable data volume and frequency

2. **ğŸ”„ Change Data Capture (CDC)**
   - Debezium connector monitors PostgreSQL Write-Ahead Log (WAL)
   - Captures INSERT, UPDATE, DELETE operations in real-time
   - Zero impact on OLTP performance (log-based CDC)
   - Publishes events to Kafka topics with full before/after state

3. **ğŸ“¨ Event Streaming**
   - Apache Kafka acts as the central nervous system
   - Decouples producers from consumers
   - Guarantees message delivery and ordering
   - Scalable distributed architecture

4. **ğŸ’¾ Data Lake Storage**
   - MinIO (S3-compatible) stores raw events in Parquet format
   - Provides cost-effective, scalable object storage
   - Acts as the "Bronze" layer (raw, immutable data)
   - Enables time-travel and data replay capabilities

5. **ğŸ­ Data Warehouse (Snowflake)**
   - **Bronze Schema**: Raw ingestion from MinIO
   - **Silver Schema**: Cleaned, standardized staging tables
   - **Gold Schema**: Business-ready marts (facts & dimensions)
   - Supports both batch and micro-batch loading

6. **ğŸ”§ Transformation Layer (dbt)**
   - **Staging Models**: Data cleansing, type casting, deduplication
   - **Fact Tables**: `fact_transactions` - grain: one row per transaction
   - **Dimension Tables**: `dim_customers`, `dim_accounts` - slowly changing dimensions
   - **Snapshots**: SCD Type-2 tracking for historical analysis
   - **Tests**: Schema validation, uniqueness, referential integrity, custom business rules

7. **âš™ï¸ Orchestration (Airflow)**
   - **DAG 1**: `minio_to_snowflake` - Incremental data ingestion
   - **DAG 2**: `scd_snapshots` - Scheduled snapshot execution
   - Automated scheduling, monitoring, and alerting
   - Retry logic and failure handling

8. **ğŸš€ CI/CD Pipeline**
   - **Continuous Integration**: dbt compile, SQL linting, automated tests
   - **Continuous Deployment**: Auto-deploy to production on merge
   - GitHub Actions workflows for quality gates

---

## âš¡ Tech Stack

### **Core Technologies**

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **OLTP Database** | PostgreSQL 15 | Source transactional system with ACID guarantees |
| **CDC Platform** | Debezium 2.2 | Log-based change data capture from Postgres WAL |
| **Event Streaming** | Apache Kafka 7.4 | Distributed event streaming platform |
| **Object Storage** | MinIO | S3-compatible data lake for raw events |
| **Data Warehouse** | Snowflake | Cloud-native columnar OLAP database |
| **Transformation** | dbt Core + dbt-snowflake | ELT framework for data modeling & testing |
| **Orchestration** | Apache Airflow 2.9 | Workflow automation and DAG scheduling |
| **Data Generation** | Python + Faker | Synthetic data simulation |
| **Containerization** | Docker + Docker Compose | Reproducible local development environment |
| **CI/CD** | GitHub Actions | Automated testing and deployment |

### **Why This Stack?**

- **Scalability**: Each component scales independently (Kafka partitions, Snowflake warehouses)
- **Reliability**: Battle-tested technologies used by Fortune 500 companies
- **Cost-Effective**: Snowflake's pay-per-use model + open-source components
- **Developer Experience**: Modern tooling with strong community support
- **Industry Standard**: Skills directly transferable to 90%+ of data engineering roles

---

## âœ… Key Features

### **Real-Time Data Capabilities**
- ğŸ”´ **Live CDC**: Captures database changes within milliseconds using log-based replication
- ğŸ¯ **Event-Driven**: Kafka ensures exactly-once delivery semantics
- âš¡ **Low Latency**: End-to-end data freshness under 30 seconds

### **Data Quality & Governance**
- âœ… **Automated Testing**: dbt tests validate data integrity at every layer
- ğŸ“Š **Schema Evolution**: Handles schema changes without pipeline breaks
- ğŸ” **Data Lineage**: Full traceability from source to dashboard
- ğŸ”’ **ACID Compliance**: Maintains consistency across distributed systems

### **Historical Tracking**
- ğŸ“… **SCD Type-2**: Snapshots capture every state change of customers/accounts
- â° **Time Travel**: Query data as it existed at any point in history
- ğŸ”„ **Audit Trail**: Complete record of who changed what and when

### **Production-Ready Engineering**
- ğŸ³ **Containerized**: Entire stack runs in Docker for consistency
- ğŸ”„ **Idempotent**: Re-runnable pipelines without side effects
- ğŸ“ˆ **Monitored**: Airflow UI provides visibility into pipeline health
- ğŸš¨ **Alerting**: Configurable notifications for failures

### **Modern Development Practices**
- ğŸ§ª **Test-Driven**: Write tests before models
- ğŸ“ **Documentation**: Auto-generated docs from dbt schema files
- ğŸ” **Version Control**: Git-based workflow for all code artifacts
- ğŸš€ **CI/CD**: Automated deployments reduce human error

---

## ğŸ“‚ Repository Structure

```
banking/
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/              # GitHub Actions CI/CD
â”‚       â”œâ”€â”€ ci.yml              # Lint, test, compile on PR
â”‚       â””â”€â”€ cd.yml              # Deploy on merge to main
â”‚
â”œâ”€â”€ banking_dbt/                # dbt Project (Transformations)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/            # Silver layer: cleaned data
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_customers.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_accounts.sql
â”‚   â”‚   â”‚   â””â”€â”€ stg_transactions.sql
â”‚   â”‚   â”œâ”€â”€ marts/              # Gold layer: business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ dimensions/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ dim_customers.sql
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ dim_accounts.sql
â”‚   â”‚   â”‚   â””â”€â”€ facts/
â”‚   â”‚   â”‚       â””â”€â”€ fact_transactions.sql
â”‚   â”‚   â””â”€â”€ sources.yml         # Source definitions
â”‚   â”œâ”€â”€ snapshots/              # SCD Type-2 tracking
â”‚   â”‚   â”œâ”€â”€ customers_snapshot.sql
â”‚   â”‚   â””â”€â”€ accounts_snapshot.sql
â”‚   â””â”€â”€ dbt_project.yml         # Project configuration
â”‚
â”œâ”€â”€ consumer/                   # Kafka â†’ MinIO
â”‚   â””â”€â”€ kafka_to_minio.py       # Consumes events, writes Parquet
â”‚
â”œâ”€â”€ data-generator/             # Synthetic Data
â”‚   â””â”€â”€ faker_generator.py      # Generates customers, accounts, txns
â”‚
â”œâ”€â”€ docker/                     # Airflow Resources
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ minio_to_snowflake_dag.py  # Ingestion DAG
â”‚   â”‚   â””â”€â”€ scd_snapshots.py           # Snapshot DAG
â”‚   â”œâ”€â”€ logs/                   # Airflow logs
â”‚   â””â”€â”€ plugins/                # Custom Airflow plugins
â”‚
â”œâ”€â”€ kafka-debezium/
â”‚   â””â”€â”€ generate_and_post_connector.py  # Debezium connector setup
â”‚
â”œâ”€â”€ postgres/
â”‚   â””â”€â”€ schema.sql              # OLTP DDL (customers, accounts, txns)
â”‚
â”œâ”€â”€ docker-compose.yml          # Local infrastructure
â”œâ”€â”€ docker-compose-arm64.yml    # Apple Silicon support
â”œâ”€â”€ dockerfile-airflow.dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### **Prerequisites**
- Docker Desktop 4.0+ with 8GB+ RAM allocated
- Docker Compose 2.0+
- Python 3.11+
- Snowflake account (free trial available)
- Git

### **1. Clone Repository**
```bash
git clone https://github.com/ASR373/banking.git
cd banking
```

### **2. Environment Setup**
```bash
# Create .env file with required credentials
cat > .env << EOF
# PostgreSQL
POSTGRES_USER=bankuser
POSTGRES_PASSWORD=bankpass
POSTGRES_DB=bankingdb

# MinIO
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin

# Airflow
AIRFLOW_DB_USER=airflow
AIRFLOW_DB_PASSWORD=airflow
AIRFLOW_DB_NAME=airflowdb

# Snowflake (add your credentials)
SNOWFLAKE_ACCOUNT=your_account
SNOWFLAKE_USER=your_user
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_WAREHOUSE=your_warehouse
SNOWFLAKE_DATABASE=BANKING_DW
EOF
```

### **3. Start Infrastructure**
```bash
# For Apple Silicon (M1/M2/M3)
docker compose -f docker-compose-arm64.yml up -d

# For Intel/AMD
docker compose up -d

# Verify all services are running
docker ps
```

### **4. Initialize Airflow**
```bash
# Initialize database
docker compose exec airflow-scheduler airflow db init

# Create admin user
docker compose exec airflow-scheduler airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com \
    --password admin
```

### **5. Set Up CDC Connector**
```bash
cd kafka-debezium
python generate_and_post_connector.py
```

### **6. Generate Sample Data**
```bash
cd data-generator
pip install -r requirements.txt
python faker_generator.py
```

### **7. Configure dbt**
```bash
cd banking_dbt
echo "banking_dw:
  target: dev
  outputs:
    dev:
      type: snowflake
      account: your_account
      user: your_user
      password: your_password
      role: SYSADMIN
      database: BANKING_DW
      warehouse: COMPUTE_WH
      schema: ANALYTICS
      threads: 4" > ~/.dbt/profiles.yml

# Test connection
dbt debug
```

### **8. Access Applications**
- **Airflow UI**: http://localhost:8080 (admin/admin)
- **MinIO Console**: http://localhost:9001 (minioadmin/minioadmin)
- **Kafka UI**: http://localhost:9021 (if Confluent Control Center enabled)

---

## ğŸ“Š Pipeline Flow

### **Step-by-Step Data Journey**

#### **Phase 1: Data Generation** âš™ï¸
```python
# faker_generator.py generates realistic data
Customer(id=1, name="John Doe", email="john@example.com")
  â†’ Account(id=101, customer_id=1, balance=5000.00, type="checking")
    â†’ Transaction(id=1001, account_id=101, amount=250.00, type="deposit")
```

#### **Phase 2: OLTP Storage** ğŸ’¾
```sql
-- Data lands in PostgreSQL with ACID guarantees
INSERT INTO customers (first_name, last_name, email) VALUES ('John', 'Doe', 'john@example.com');
INSERT INTO accounts (customer_id, account_type, balance) VALUES (1, 'checking', 5000.00);
INSERT INTO transactions (account_id, amount, txn_type) VALUES (101, 250.00, 'deposit');
```

#### **Phase 3: CDC Capture** ğŸ”
```json
// Debezium captures WAL entry
{
  "op": "c",  // create
  "after": {
    "id": 1001,
    "account_id": 101,
    "amount": 250.00,
    "txn_type": "deposit",
    "created_at": "2024-02-15T10:30:00Z"
  }
}
```

#### **Phase 4: Kafka Stream** ğŸ“¨
```
Topic: banking.public.transactions
Partition: 0
Offset: 12345
Message: [JSON payload above]
```

#### **Phase 5: MinIO Storage** ğŸ“¦
```
s3://banking-raw/
  â””â”€â”€ transactions/
      â””â”€â”€ 2024/02/15/
          â””â”€â”€ transactions_20240215_103000.parquet
```

#### **Phase 6: Snowflake Ingestion** â„ï¸
```sql
-- Airflow DAG copies to Bronze
COPY INTO BRONZE.RAW_TRANSACTIONS
FROM @BANKING_STAGE/transactions/2024/02/15/
FILE_FORMAT = (TYPE = PARQUET);
```

#### **Phase 7: dbt Transformations** ğŸ”§
```sql
-- Staging (Silver)
CREATE OR REPLACE TABLE SILVER.STG_TRANSACTIONS AS
SELECT 
    id,
    account_id,
    amount,
    txn_type,
    created_at,
    CURRENT_TIMESTAMP() AS ingested_at
FROM BRONZE.RAW_TRANSACTIONS;

-- Fact Table (Gold)
CREATE OR REPLACE TABLE GOLD.FACT_TRANSACTIONS AS
SELECT 
    t.id AS transaction_key,
    a.account_key,
    c.customer_key,
    t.amount,
    t.txn_type,
    t.created_at AS transaction_date
FROM SILVER.STG_TRANSACTIONS t
JOIN GOLD.DIM_ACCOUNTS a ON t.account_id = a.account_id
JOIN GOLD.DIM_CUSTOMERS c ON a.customer_id = c.customer_id;
```

#### **Phase 8: Historical Snapshots** ğŸ“¸
```sql
-- dbt Snapshot captures SCD Type-2
SELECT * FROM GOLD.DIM_CUSTOMERS_SNAPSHOT
WHERE customer_id = 1;

-- Result shows historical changes:
| customer_key | customer_id | email              | dbt_valid_from | dbt_valid_to |
|--------------|-------------|--------------------|----------------|--------------|
| 1            | 1           | john@old.com       | 2024-01-01     | 2024-02-15   |
| 2            | 1           | john@example.com   | 2024-02-15     | NULL         |
```

---

## ğŸ¯ Data Modeling

### **Dimensional Model**

```
GOLD Layer (Star Schema):

                    DIM_CUSTOMERS
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ customer_key   â”‚
                    â”‚ customer_id    â”‚
                    â”‚ first_name     â”‚
                    â”‚ last_name      â”‚
                    â”‚ email          â”‚
                    â”‚ created_at     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚ 1:M
                             â”‚
    DIM_ACCOUNTS             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  FACT_TRANSACTIONS
    â”‚ account_key    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ account_id     â”‚â—„â”€â”¤ transaction_key    â”‚
    â”‚ customer_id    â”‚  â”‚ account_key (FK)   â”‚
    â”‚ account_type   â”‚  â”‚ customer_key (FK)  â”‚
    â”‚ balance        â”‚  â”‚ amount             â”‚
    â”‚ currency       â”‚  â”‚ txn_type           â”‚
    â”‚ created_at     â”‚  â”‚ transaction_date   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ status             â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **dbt Model Lineage**

```
Sources (PostgreSQL)
    â”‚
    â”œâ”€â”€ customers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º stg_customers â”€â”€â”€â”€â–º dim_customers â”€â”€â”€â”€â–º customers_snapshot
    â”‚                                                     â”‚
    â”œâ”€â”€ accounts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º stg_accounts â”€â”€â”€â”€â”€â–º dim_accounts â”€â”€â”€â”€â”€â”€â–º accounts_snapshot
    â”‚                                                     â”‚
    â””â”€â”€ transactions â”€â”€â”€â”€â”€â”€â”€â–º stg_transactions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º fact_transactions
```

---

## ğŸ§ª Testing Strategy

### **dbt Tests Implemented**

```yaml
# models/schema.yml
models:
  - name: fact_transactions
    tests:
      - dbt_utils.recency:
          datepart: day
          field: transaction_date
          interval: 1
    columns:
      - name: transaction_key
        tests:
          - unique
          - not_null
      - name: account_key
        tests:
          - relationships:
              to: ref('dim_accounts')
              field: account_key
      - name: amount
        tests:
          - not_null
          - dbt_expectations.expect_column_values_to_be_between:
              min_value: 0
              max_value: 1000000
```

### **Test Coverage**
- âœ… Schema validation (column names, data types)
- âœ… Uniqueness constraints (primary keys)
- âœ… Referential integrity (foreign keys)
- âœ… Null checks (required fields)
- âœ… Value range validation (amounts, dates)
- âœ… Data freshness (recency checks)
- âœ… Custom business rules (balance >= 0)

---

## ğŸ”„ CI/CD Workflow

### **Continuous Integration** (on PR)
```yaml
name: CI
on: [pull_request]
jobs:
  dbt-tests:
    runs-on: ubuntu-latest
    steps:
      - Checkout code
      - Install dbt
      - Run dbt compile
      - Run dbt test
      - SQL lint check
      - Security scan
```

### **Continuous Deployment** (on merge)
```yaml
name: CD
on:
  push:
    branches: [main]
jobs:
  deploy-production:
    runs-on: ubuntu-latest
    steps:
      - Deploy Airflow DAGs
      - Run dbt run (production)
      - Run dbt test (production)
      - Update documentation
      - Notify Slack
```

---

## ğŸ“ˆ Monitoring & Observability

### **Airflow Monitoring**
- ğŸ“Š DAG execution times and success rates
- ğŸš¨ Failure alerts via email/Slack
- ğŸ“‰ Task duration trends
- ğŸ”„ Retry attempts and SLA misses

### **dbt Monitoring**
- âœ… Test pass/fail rates
- â±ï¸ Model execution times
- ğŸ“Š Row count anomaly detection
- ğŸ” Data quality scores

### **Snowflake Monitoring**
- ğŸ’° Credit consumption tracking
- ğŸš€ Query performance analysis
- ğŸ“¦ Storage growth trends
- ğŸ‘¥ User access patterns

---

## ğŸ” Security & Compliance

- ğŸ”’ **Secrets Management**: All credentials stored in environment variables
- ğŸ›¡ï¸ **Network Isolation**: Services communicate through Docker internal networks
- ğŸ”‘ **Role-Based Access**: Snowflake roles for least privilege
- ğŸ“ **Audit Logging**: Full history of data changes via CDC
- ğŸ” **Encryption**: Data encrypted at rest (Snowflake) and in transit (TLS)

---

## ğŸš§ Roadmap

### **Completed** âœ…
- [x] End-to-end CDC pipeline
- [x] dbt dimensional modeling
- [x] Airflow orchestration
- [x] SCD Type-2 snapshots
- [x] CI/CD with GitHub Actions
- [x] Docker containerization

### **In Progress** ğŸ—ï¸
- [ ] Real-time dashboards (Streamlit/Tableau)
- [ ] Data quality framework (Great Expectations)
- [ ] Cost optimization analysis

### **Future Enhancements** ğŸ”®
- [ ] Machine learning models (fraud detection)
- [ ] GraphQL API for data access
- [ ] Multi-region deployment
- [ ] Kubernetes orchestration

---

## ğŸ“š Learning Resources

If you're new to any of these technologies, here are some great starting points:

- **dbt**: [dbt Learn](https://courses.getdbt.com/)
- **Airflow**: [Apache Airflow Tutorial](https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html)
- **Kafka**: [Kafka Quickstart](https://kafka.apache.org/quickstart)
- **Snowflake**: [Snowflake Hands-On Essentials](https://www.snowflake.com/virtual-hands-on-lab/)
- **Debezium**: [Debezium Tutorial](https://debezium.io/documentation/reference/tutorial.html)

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## ğŸ‘¤ Author

**Adith Sreeram Arjunan Sivakumar**

- GitHub: [@ASR373](https://github.com/ASR373)
- LinkedIn: [Your LinkedIn Profile](#)
- Email: your.email@example.com

---

## ğŸ™ Acknowledgments

- Inspired by real-world data engineering challenges in the financial services industry
- Built with open-source technologies from amazing communities
- Special thanks to the dbt, Airflow, and Kafka communities for excellent documentation

---

<div align="center">

**â­ If this project helped you, please star it on GitHub! â­**

Made with â¤ï¸ by [ASR373](https://github.com/ASR373)

</div>
